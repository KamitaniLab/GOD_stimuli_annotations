# dataset_captions

This repository shares only the captions of the ImageNetTraining stimulus dataset (also called GOD) used in the following KamitaniLab papers. Please note that the image itself cannot be shared due to copyright problems. If you would like to need the data, so please contact us from [here](https://forms.gle/ujvA34948Xg49jdn9).

1. Horikawa, T., & Kamitani, Y. (2017). Generic decoding of seen and imagined objects using hierarchical visual features. Nature Communications, 8, 15037. https://doi.org/10.1038/ncomms15037
2. Shen, G., Horikawa, T., Majima, K., & Kamitani, Y. (2019). Deep image reconstruction from human brain activity. PLOS Computational Biology, 15(1), 1006633. https://doi.org/10.1371/journal.pcbi.1006633
3. Shen, G., Dwivedi, K., Majima, K., Horikawa, T., & Kamitani, Y. (2019). End-to-End Deep Image Reconstruction From Human Brain Activity. Frontiers in Computational Neuroscience, 13, 21. https://doi.org/10.3389/fncom.2019.00021

The caption data was collected in 2018 using Amazon Mechanical Turk (AMT), a crowdsourcing service. We use the following two worker filtering when collecting captions.
1. Masters has been granted.
2. HIT Approval Rate (%) for all Requesters' HITs greater than 95.

